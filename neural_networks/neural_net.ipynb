{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 785)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.genfromtxt('mnist_train.csv',delimiter=',')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[:,1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1281a71d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test=data[0]\n",
    "test = (np.reshape(test, (28, 28))).astype(np.uint8)\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "    def __init__(self,num_hidden_layers,num_neuron,learning_rate):\n",
    "        self.num_input=7000\n",
    "        self.dimension=784\n",
    "        self.out=10\n",
    "        self.layers=[]\n",
    "        self.bias=[]\n",
    "        self.weights=[]\n",
    "        self.num_hidden_layers=num_hidden_layers\n",
    "        self.num_neuron=num_neuron\n",
    "        self.learning_rate=learning_rate\n",
    "        self.d_layers=[]\n",
    "        self.d_weights=[]\n",
    "        self.d_bias=[]\n",
    "        self.x_train=np.zeros((7000,784))\n",
    "        self.y_train=np.zeros(7000)\n",
    "        self.y_train_hot=np.zeros((7000,10))\n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    def getdata(self):\n",
    "        data=np.genfromtxt('mnist_train.csv',delimiter=',')\n",
    "        self.y_train=data[:,0]\n",
    "        self.x_train=data[:,1:]/255\n",
    "        self.y_train=self.y_train.astype(int)\n",
    "        self.y_train_hot[range(self.num_input),self.y_train]=1\n",
    "    def weight_initialze(self):\n",
    "        self.getdata()\n",
    "        alpha=0.1\n",
    "        W=alpha*np.random.randn(self.dimension,self.num_neuron)\n",
    "        self.weights.append(W)\n",
    "        b=np.zeros((1,self.num_neuron))\n",
    "        self.bias.append(b)\n",
    "        for i in range(1,self.num_hidden_layers):\n",
    "            W=alpha*np.random.randn(self.num_neuron,self.num_neuron)\n",
    "            self.weights.append(W)\n",
    "            b=np.zeros((1,self.num_neuron))\n",
    "            self.bias.append(b)\n",
    "        W=alpha*np.random.randn(self.num_neuron,self.out)\n",
    "        self.weights.append(W)\n",
    "        b=np.zeros((1,self.out))\n",
    "        self.bias.append(b)      \n",
    "    def forward(self):\n",
    "        j=0\n",
    "        self.layers=[]\n",
    "        self.layers.append(np.dot(self.x_train,self.weights[j])+self.bias[j])\n",
    "        for i in range(1,self.num_hidden_layers):\n",
    "            j+=1\n",
    "            self.layers.append(self.relu((np.dot(self.layers[j-1],self.weights[j])+self.bias[j])))\n",
    "        x=np.exp((np.dot(self.layers[j],self.weights[j+1]))+self.bias[j+1])\n",
    "        x=x/np.sum(x,axis=1,keepdims=True)\n",
    "        self.layers.append(x)\n",
    "    def back(self):\n",
    "        j=0\n",
    "        k=self.num_hidden_layers\n",
    "        self.d_layers=[]\n",
    "        self.d_weights=[]\n",
    "        self.d_bias=[]\n",
    "        self.d_layers.append((self.layers[k]-self.y_train_hot)/self.num_input)\n",
    "        self.d_weights.append(np.dot((self.layers[k-1]).T,self.d_layers[j]))\n",
    "        self.d_bias.append(np.sum(self.d_layers[j],axis=0,keepdims=True))\n",
    "        for i in range(1,self.num_hidden_layers):\n",
    "            j+=1\n",
    "            self.d_layers.append(np.dot(self.d_layers[j-1],(self.weights[k]).T))\n",
    "            W=self.d_layers[j]\n",
    "            W[self.layers[k-1]<=0]=0\n",
    "            self.d_layers[j]=W\n",
    "            k-=1\n",
    "            self.d_weights.append(np.dot((self.layers[k-1]).T,self.d_layers[j]))\n",
    "            self.d_bias.append(np.sum(self.d_layers[j],axis=0,keepdims=True))\n",
    "        self.d_layers.append(np.dot(self.d_layers[j],(self.weights[k]).T))\n",
    "        self.d_weights.append(np.dot((self.x_train).T,self.d_layers[j+1]))\n",
    "        self.d_bias.append(np.sum(self.d_layers[j+1],axis=0,keepdims=True))\n",
    "    def weight_updation(self):\n",
    "        k=self.num_hidden_layers\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.weights[i]+=-self.learning_rate*self.d_weights[k]\n",
    "            self.bias[i]+=-self.learning_rate*self.d_bias[k]\n",
    "            k=k-1\n",
    "    def error(self):\n",
    "        loss=0.5*np.sum(np.square((self.layers[self.num_hidden]-self.Y_train_hot)))/7000\n",
    "        return (loss)\n",
    "    def train(self):\n",
    "        epoch=1000           # change the epochs to see the variation\n",
    "        self.weight_initialze()\n",
    "        for i in range(epoch):\n",
    "            self.forward()\n",
    "            self.back()\n",
    "            self.weight_updation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=neural_net(3,300,.1)\n",
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,num_hidden,num_neuron,learning_rate):\n",
    "        self.num_input=7000                  #took the 7000 samples given for training\n",
    "        self.dimension=784\n",
    "        self.num_out=10                      #output is 10 classes\n",
    "        self.layers=[]\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.num_hidden=num_hidden\n",
    "        self.num_neuron=num_neuron\n",
    "        self.learning_rate=learning_rate\n",
    "        self.X_train=np.zeros((7000,784))\n",
    "        self.Y_train=np.zeros(7000)\n",
    "        self.d_weights=[]\n",
    "        self.d_bias=[]\n",
    "        self.d_layers=[]\n",
    "        self.Y_train_hot=np.zeros((7000,10))\n",
    "\n",
    "    def relu(self,x):                         #relu for nonlinearity\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def get_data(self):                       # getting the data\n",
    "        Data= np.genfromtxt(r'mnist_train.csv', delimiter=',')\n",
    "        self.Y_train=Data[:,0]\n",
    "        self.X_train=Data[:,1:]/255\n",
    "        self.Y_train=self.Y_train.astype(int)\n",
    "        self.Y_train_hot[range(self.num_input),self.Y_train]=1          #one hot representation of training examples\n",
    "\n",
    "    def weight_inialization(self):           #initialising the weights for each layer performance can change with values of alpha\n",
    "        self.get_data()\n",
    "        alpha=.1\n",
    "        W=alpha*np.random.randn(self.dimension,self.num_neuron)\n",
    "        self.weights.append(W)\n",
    "        b=np.zeros((1,self.num_neuron))\n",
    "        self.bias.append(b)\n",
    "        for i in range(1,self.num_hidden):\n",
    "            W=alpha*np.random.randn(self.num_neuron,self.num_neuron)\n",
    "            self.weights.append(W)\n",
    "            b=np.zeros((1,self.num_neuron))\n",
    "            self.bias.append(b)\n",
    "        W=alpha*np.random.randn(self.num_neuron,self.num_out)\n",
    "        self.weights.append(W)\n",
    "        b=np.zeros((1,self.num_out))\n",
    "        self.bias.append(b)\n",
    "\n",
    "    def forward_pass(self):                 #normal forward pass\n",
    "        j=0\n",
    "        self.layers=[]\n",
    "        self.layers.append(np.dot(self.X_train,self.weights[j])+self.bias[j])\n",
    "        for i in range(1,self.num_hidden):\n",
    "            j=j+1\n",
    "            self.layers.append(self.relu((np.dot(self.layers[j-1],self.weights[j])+self.bias[j])))\n",
    "        x=np.exp((np.dot(self.layers[j],self.weights[j+1]))+self.bias[j+1])\n",
    "        x=x/np.sum(x,axis=1,keepdims=True)\n",
    "        self.layers.append(x)\n",
    "\n",
    "    def back_pass(self):                   #backprop\n",
    "        j=0\n",
    "        k=self.num_hidden\n",
    "        self.d_layers=[]\n",
    "        self.d_weights=[]\n",
    "        self.d_bias=[]                                                           #first layer and last layer backprop have to explicitly written\n",
    "        self.d_layers.append((self.layers[k]-self.Y_train_hot)/self.num_input)\n",
    "        self.d_weights.append(np.dot((self.layers[k-1]).T,self.d_layers[j]))\n",
    "        self.d_bias.append(np.sum(self.d_layers[j],axis=0,keepdims=True))\n",
    "        for i in range(1,self.num_hidden):\n",
    "            j=j+1\n",
    "            self.d_layers.append(np.dot(self.d_layers[j-1],(self.weights[k]).T))\n",
    "            W=self.d_layers[j]\n",
    "            W[self.layers[k-1]<=0]=0\n",
    "            self.d_layers[j]=W\n",
    "            k=k-1\n",
    "            self.d_weights.append(np.dot((self.layers[k-1]).T,self.d_layers[j]))\n",
    "            self.d_bias.append(np.sum(self.d_layers[j],axis=0,keepdims=True))\n",
    "        self.d_layers.append(np.dot(self.d_layers[j],(self.weights[k]).T))\n",
    "        self.d_weights.append(np.dot((self.X_train).T,self.d_layers[j+1]))\n",
    "        self.d_bias.append(np.sum(self.d_layers[j+1],axis=0,keepdims=True))\n",
    "\n",
    "    def weight_updation(self):      #updating the weight\n",
    "        k=self.num_hidden\n",
    "        for i in range(self.num_hidden):\n",
    "            self.weights[i]+=-self.learning_rate*self.d_weights[k]\n",
    "            self.bias[i]+=-self.learning_rate*self.d_bias[k]\n",
    "            k=k-1\n",
    "\n",
    "    def error(self):\n",
    "        loss=0.5*np.sum(np.square((self.layers[self.num_hidden]-self.Y_train_hot)))/7000\n",
    "        return (loss)\n",
    "\n",
    "    def accuracy(self):\n",
    "        count=0\n",
    "        predicted=np.argmax(self.layers[self.num_hidden],axis=1)\n",
    "        for i in range(self.num_input):\n",
    "            if(predicted[i]==self.Y_train[i]):\n",
    "                count+=1\n",
    "        count=count/self.num_input\n",
    "        print(count)\n",
    "\n",
    "    def train(self):\n",
    "        epoch=1000           # change the epochs to see the variation\n",
    "        self.weight_inialization()\n",
    "        for i in range(epoch):\n",
    "            self.forward_pass()\n",
    "            self.back_pass()\n",
    "            self.weight_updation()\n",
    "            #print(self.error())\n",
    "\n",
    "    def test(self):\n",
    "        test_data= np.genfromtxt(r'C:\\Users\\hari\\ml_assignment\\mnist_test.csv', delimiter=',')\n",
    "        test_data=test_data/255\n",
    "        j=0\n",
    "        self.layers=[]\n",
    "        self.layers.append(np.dot(test_data,self.weights[j])+self.bias[j])\n",
    "        for i in range(1,self.num_hidden):\n",
    "            j=j+1\n",
    "            self.layers.append(self.relu((np.dot(self.layers[j-1],self.weights[j])+self.bias[j])))\n",
    "        x=np.exp((np.dot(self.layers[j],self.weights[j+1]))+self.bias[j+1])\n",
    "        x=x/np.sum(x,axis=1,keepdims=True)\n",
    "        self.layers.append(x)\n",
    "        predicted=np.argmax(self.layers[self.num_hidden],axis=1)\n",
    "        predicted=predicted.astype(int)\n",
    "        np.savetxt(\"test_result.csv\", predicted, fmt='%10.0f', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net1=NeuralNet(2,300,.1) #declare as NeuralNet(number of hidden layers,number of neurons in hidden layer,learning rate)\n",
    "Net1.train()\n",
    "Net1.accuracy()           #acuuracy in training_set\n",
    "Net1.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901428571428571\n"
     ]
    }
   ],
   "source": [
    "Net1.accuracy()           #acuuracy in training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
