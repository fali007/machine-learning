{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 453 characters, 33 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 33) (100, 100) (33, 100)\n"
     ]
    }
   ],
   "source": [
    "print(Wxh.shape,Whh.shape,Why.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " gw,yfyNou-, sE,ygxrfr,:aopTwlxavt,febtac-wdehCwgCgbehAlcwAe sNp.ccEmeCnpxuAyfanp,vcbeAxonhkT,fAcmve  ulnbAhmr,e-ApsbeiutknffC.xxN o:m:mramedEngNtu tEbx:hr--Er kaf.tpuCEhctp,llvEmmbeseoeawnTwNioweoblhC \n",
      "----\n",
      "iter 0, loss: 87.412701\n",
      "iter 100, loss: 88.925599\n",
      "iter 200, loss: 87.882925\n",
      "iter 300, loss: 86.687820\n",
      "iter 400, loss: 85.290256\n",
      "iter 500, loss: 83.740681\n",
      "iter 600, loss: 81.912672\n",
      "iter 700, loss: 80.031675\n",
      "iter 800, loss: 78.084772\n",
      "iter 900, loss: 76.163848\n",
      "----\n",
      " lTf tl viupree al ss anavetiawsfcols t aley selysiod lcuxkd sind o pivluroEapr lutoomanah: drod datoucps. feoioufwodotf pimy p urodimal  cwc ceocnlt thudey ooifo hinnlsuirelrhas. leulop n dss oncbaom  \n",
      "----\n",
      "iter 1000, loss: 74.298882\n",
      "iter 1100, loss: 72.402354\n",
      "iter 1200, loss: 70.528724\n",
      "iter 1300, loss: 68.692699\n",
      "iter 1400, loss: 66.916666\n",
      "iter 1500, loss: 65.143232\n",
      "iter 1600, loss: 63.390896\n",
      "iter 1700, loss: 61.639149\n",
      "iter 1800, loss: 59.988222\n",
      "iter 1900, loss: 58.360179\n",
      "----\n",
      " a d erkc d tmesy eivearts t imiriugles, theuravetw alt bindor. sonnas al sio s. una tsimabitssinhave s ihurks totill six abser. functs y lutlora botlere wer. Nelrove s fola aw fgncua ntnerthoro olilal \n",
      "----\n",
      "iter 2000, loss: 56.744077\n",
      "iter 2100, loss: 55.230175\n",
      "iter 2200, loss: 53.665250\n",
      "iter 2300, loss: 52.231681\n",
      "iter 2400, loss: 50.841363\n",
      "iter 2500, loss: 49.497836\n",
      "iter 2600, loss: 48.265026\n",
      "iter 2700, loss: 47.024425\n",
      "iter 2800, loss: 45.885597\n",
      "iter 2900, loss: 44.671266\n",
      "----\n",
      " ivetw m dinctiony seapio fundocgthaf dons awhaEale weageine wea d tiory eealeiimilamilopiomilane wexelonam thane minereawhta illret tiol Neut fld nave ceighanliwe s onaoca onturmapra llvlo dntots fnd  \n",
      "----\n",
      "iter 3000, loss: 43.600702\n",
      "iter 3100, loss: 42.448176\n",
      "iter 3200, loss: 41.430988\n",
      "iter 3300, loss: 40.465152\n",
      "iter 3400, loss: 39.458865\n",
      "iter 3500, loss: 38.576814\n",
      "iter 3600, loss: 37.592807\n",
      "iter 3700, loss: 36.738920\n",
      "iter 3800, loss: 35.939204\n",
      "iter 3900, loss: 35.067962\n",
      "----\n",
      " . bloles  ne wilury snadheup onnduhclarcur:lrom Neurflrio works ft ory serpurmork  thun-n. Apie f rmilh venetwork ild tioth rm Nesy furils et orotorenfurons ot  r: theseass sroneurkhe r vral frtionona \n",
      "----\n",
      "iter 4000, loss: 34.304787\n",
      "iter 4100, loss: 33.561708\n",
      "iter 4200, loss: 32.908789\n",
      "iter 4300, loss: 32.225121\n",
      "iter 4400, loss: 31.430192\n",
      "iter 4500, loss: 30.634200\n",
      "iter 4600, loss: 30.151487\n",
      "iter 4700, loss: 29.449980\n",
      "iter 4800, loss: 28.880072\n",
      "iter 4900, loss: 28.215161\n",
      "----\n",
      " rom the ptitherecril wheseare weeres eivethewianpte de foralldiandiod im Nets andild lancnay ilmyit similareureurane in: ardioformarim Nearolos onc inareime ill tionallctiodasiy fonctionale to sesss a \n",
      "----\n",
      "iter 5000, loss: 27.701537\n",
      "iter 5100, loss: 27.101987\n",
      "iter 5200, loss: 26.713002\n",
      "iter 5300, loss: 26.165691\n",
      "iter 5400, loss: 25.613110\n",
      "iter 5500, loss: 25.149477\n",
      "iter 5600, loss: 24.743911\n",
      "iter 5700, loss: 24.279159\n",
      "iter 5800, loss: 23.804978\n",
      "iter 5900, loss: 23.202797\n",
      "----\n",
      " Netw and hery hes ar toima il ind ot are ol Neuronclines pte faby. slare  ahes fut o :er. ary simcthonetworms able wixblol ut ork woam th ll nal Neuroufronkwotheral siapryy s c arot heve s wo olclimil \n",
      "----\n",
      "iter 6000, loss: 22.891192\n",
      "iter 6100, loss: 22.424629\n",
      "iter 6200, loss: 21.839613\n",
      "iter 6300, loss: 21.474596\n",
      "iter 6400, loss: 21.233516\n",
      "iter 6500, loss: 20.824952\n",
      "iter 6600, loss: 20.345984\n",
      "iter 6700, loss: 20.003759\n",
      "iter 6800, loss: 19.654738\n",
      "iter 6900, loss: 19.203734\n",
      "----\n",
      "  binmarinaswiom ond nable neurof pte scorathete tixpronor Neurexpte ther. frod tos ca nanghaneile o Neur: ry Nearnauwithey ctiglcom Network ses a eareile opuaprek har to onascas to mivhoghaneureas a l \n",
      "----\n",
      "iter 7000, loss: 18.971295\n",
      "iter 7100, loss: 18.669064\n",
      "iter 7200, loss: 18.341357\n",
      "iter 7300, loss: 17.903023\n",
      "iter 7400, loss: 17.474519\n",
      "iter 7500, loss: 17.047686\n",
      "iter 7600, loss: 16.701377\n",
      "iter 7700, loss: 16.550115\n",
      "iter 7800, loss: 16.153686\n",
      "iter 7900, loss: 15.883135\n",
      "----\n",
      " ts and binforeime ind oup of neilect thau s ablar. And arodhes hone rot heartionallchane ss sce ptimapexchapte ptimapreduthes a chechas are wherito ory And to araupiral che hane rod aanctiogwonNeural  \n",
      "----\n",
      "iter 8000, loss: 15.510016\n",
      "iter 8100, loss: 15.398763\n",
      "iter 8200, loss: 14.999345\n",
      "iter 8300, loss: 14.685075\n",
      "iter 8400, loss: 14.873003\n",
      "iter 8500, loss: 14.505747\n",
      "iter 8600, loss: 14.420956\n",
      "iter 8700, loss: 14.136656\n",
      "iter 8800, loss: 13.859673\n",
      "iter 8900, loss: 13.587344\n",
      "----\n",
      " Convolrease glarith ard pte odilery snd ome hane tito oras frnillillrssse ghane fapiom Netwe previous sot arct wotf nelletwe ptifune e vewilmilassi one the op neure fawee ge far ao optoa ss cnandthave \n",
      "----\n",
      "iter 9000, loss: 13.383603\n",
      "iter 9100, loss: 13.051498\n",
      "iter 9200, loss: 12.881861\n",
      "iter 9300, loss: 12.662232\n",
      "iter 9400, loss: 12.406243\n",
      "iter 9500, loss: 12.181862\n",
      "iter 9600, loss: 12.310559\n",
      "iter 9700, loss: 12.002382\n",
      "iter 9800, loss: 11.705449\n",
      "iter 9900, loss: 11.419313\n",
      "----\n",
      " inal Network ond thes. Eolav ous ond imiver: othane olass Neurav hane lnd to ceas inar ses. Eaptionary Neural Networks are w onal Network s mighes a gle divfone rod tot wry se piot wimilasct onmress a \n",
      "----\n",
      "iter 10000, loss: 11.275133\n",
      "iter 10100, loss: 10.966118\n",
      "iter 10200, loss: 10.714297\n",
      "iter 10300, loss: 11.014043\n",
      "iter 10400, loss: 10.762554\n",
      "iter 10500, loss: 10.443517\n",
      "iter 10600, loss: 10.312035\n",
      "iter 10700, loss: 10.331084\n",
      "iter 10800, loss: 10.118273\n",
      "iter 10900, loss: 9.853943\n",
      "----\n",
      " onelleime exeeuralctiondild havctiold ame navlongrod to s, . The fathts inary ond to ilar:iffry old to Anal Neural Networks s mesimigilnco chapredrod al Neurons onale on haviaas a doronale eave cout o \n",
      "----\n",
      "iter 11000, loss: 9.788016\n",
      "iter 11100, loss: 9.570142\n",
      "iter 11200, loss: 9.277756\n",
      "iter 11300, loss: 9.097093\n",
      "iter 11400, loss: 9.154509\n",
      "iter 11500, loss: 8.923463\n",
      "iter 11600, loss: 8.655908\n",
      "iter 11700, loss: 8.503336\n",
      "iter 11800, loss: 8.271215\n",
      "iter 11900, loss: 8.020521\n",
      "----\n",
      " xpteilass Ne up they co Neredreas, Neoreareime exfltwor. Tre sio fry Neural Networks tiable whale woay fothermavey stionane wilprethe peaghass score from the peime eareime inpurfurm scory simcla s old \n",
      "----\n",
      "iter 12000, loss: 8.046500\n",
      "iter 12100, loss: 8.275378\n",
      "iter 12200, loss: 8.056713\n",
      "iter 12300, loss: 7.806163\n",
      "iter 12400, loss: 7.687263\n",
      "iter 12500, loss: 7.550336\n",
      "iter 12600, loss: 7.326899\n",
      "iter 12700, loss: 7.221676\n",
      "iter 12800, loss: 7.134017\n",
      "iter 12900, loss: 6.939109\n",
      "----\n",
      " . And they still have m ear to il fwsiy Ne u Therndifurms a s ondils wotherenteigeaglr: the ils eadituron ravptesct ald theud of neurnl biaserearecley teiner. And they arol we ch ve weifar sowolopioap \n",
      "----\n",
      "iter 13000, loss: 7.358657\n",
      "iter 13100, loss: 7.206390\n",
      "iter 13200, loss: 6.979311\n",
      "iter 13300, loss: 6.799404\n",
      "iter 13400, loss: 6.779433\n",
      "iter 13500, loss: 6.850570\n",
      "iter 13600, loss: 6.646060\n",
      "iter 13700, loss: 6.437229\n",
      "iter 13800, loss: 6.319549\n",
      "iter 13900, loss: 6.123214\n",
      "----\n",
      " rom the raw in llilf thes chaprevcod hable we ge ptim perd therentiable score function: fromy fa s that her imaght seceives some e veive neorets and wots, pte bialiauls a dormchat to old exeimy. there \n",
      "----\n",
      "iter 14000, loss: 5.928882\n",
      "iter 14100, loss: 6.101608\n",
      "iter 14200, loss: 6.137661\n",
      "iter 14300, loss: 5.951782\n",
      "iter 14400, loss: 6.002287\n",
      "iter 14500, loss: 6.288563\n",
      "iter 14600, loss: 6.326902\n",
      "iter 14700, loss: 6.121011\n",
      "iter 14800, loss: 5.916686\n",
      "iter 14900, loss: 5.724590\n",
      "----\n",
      " score madithe optithawhes function:ifunills chctht it  n: fr. And they still have a nolilary sxelard ble we piot wimilar ary pre fwwiou s it ses itss sil soctwormssime dinmarity. The weege s arworm pr \n",
      "----\n",
      "iter 15000, loss: 5.536877\n",
      "iter 15100, loss: 5.389179\n",
      "iter 15200, loss: 5.213680\n",
      "iter 15300, loss: 5.052047\n",
      "iter 15400, loss: 4.897667\n",
      "iter 15500, loss: 4.753297\n",
      "iter 15600, loss: 4.621965\n",
      "iter 15700, loss: 4.528256\n",
      "iter 15800, loss: 5.835810\n",
      "iter 15900, loss: 5.949525\n",
      "----\n",
      "  rat ar Eoctione ral till have cearhty ore networks ardilll ondres as old and optionine whole neurondrod tiabiang ther. And theural Networks ard the prome il ses arime ih l slc as ond exeims ap : And  \n",
      "----\n",
      "iter 16000, loss: 5.762187\n",
      "iter 16100, loss: 5.533968\n",
      "iter 16200, loss: 5.312825\n",
      "iter 16300, loss: 5.103970\n",
      "iter 16400, loss: 4.909929\n",
      "iter 16500, loss: 4.731513\n",
      "iter 16600, loss: 4.563935\n",
      "iter 16700, loss: 4.406344\n",
      "iter 16800, loss: 4.260222\n",
      "iter 16900, loss: 4.126946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ts, performs a not pron Each neuron les mrmilar to ordinary Neiral areclimile fawe fasss ot or. ut ble od Networks a dit st theurav thes andit ouchasss ablore made frnm labial bleigdild ther. And to i \n",
      "----\n",
      "iter 17000, loss: 4.000318\n",
      "iter 17100, loss: 4.716504\n",
      "iter 17200, loss: 4.812916\n",
      "iter 17300, loss: 4.768997\n",
      "iter 17400, loss: 4.593575\n",
      "iter 17500, loss: 4.422381\n",
      "iter 17600, loss: 4.259893\n",
      "iter 17700, loss: 4.109305\n",
      "iter 17800, loss: 3.971933\n",
      "iter 17900, loss: 3.839396\n",
      "----\n",
      " Convolrod it ord ble weif peccork s s it s chaprenctiorms ane rad thale leApfane rnd axple fun ofy. The werfunction: from the reinearetyeineareafawes. Eaptecd tlorallchapfonetwofupunchavionchaprenctio \n",
      "----\n",
      "iter 18000, loss: 3.897572\n",
      "iter 18100, loss: 4.588131\n",
      "iter 18200, loss: 4.785846\n",
      "iter 18300, loss: 4.746474\n",
      "iter 18400, loss: 4.591559\n",
      "iter 18500, loss: 4.405829\n",
      "iter 18600, loss: 4.224680\n",
      "iter 18700, loss: 4.057462\n",
      "iter 18800, loss: 3.897307\n",
      "iter 18900, loss: 3.751604\n",
      "----\n",
      " vt one rnt ardi lxsiolill sigl ral Neural Networks from ll have a loscer: function: from the raw image pixels me ot ord op able dim prod wia worm pion wes. Each neuresses. Each ne up of neuron thale a \n",
      "----\n",
      "iter 19000, loss: 3.615519\n",
      "iter 19100, loss: 3.489500\n",
      "iter 19200, loss: 3.374823\n",
      "iter 19300, loss: 3.268356\n",
      "iter 19400, loss: 3.168534\n",
      "iter 19500, loss: 3.077257\n",
      "iter 19600, loss: 2.994129\n",
      "iter 19700, loss: 2.912704\n",
      "iter 19800, loss: 2.838585\n",
      "iter 19900, loss: 3.565274\n",
      "----\n",
      " ole torill expresses fry Neural Netwocs thasprec arnm pareiy ares are ss s ore ff tiable score fune chaptes it sime ima aivs, pte ss s ores ao ses a single diffey stitionctionally follof nare ss stile \n",
      "----\n",
      "iter 20000, loss: 3.910771\n",
      "iter 20100, loss: 4.015604\n",
      "iter 20200, loss: 3.969445\n",
      "iter 20300, loss: 3.846914\n",
      "iter 20400, loss: 3.713060\n",
      "iter 20500, loss: 3.585483\n",
      "iter 20600, loss: 3.457154\n",
      "iter 20700, loss: 3.332975\n",
      "iter 20800, loss: 3.216284\n",
      "iter 20900, loss: 3.107344\n",
      "----\n",
      " xpressss a dot product and optionencot prod sxelardormineril bias. Each ssivend therof pe ltiond to class score madithe whtchavl have le olllao stional Neup tots itas frod to chapte ce netweime inas s \n",
      "----\n",
      "iter 21000, loss: 3.006891\n",
      "iter 21100, loss: 2.913940\n",
      "iter 21200, loss: 2.825866\n",
      "iter 21300, loss: 2.745004\n",
      "iter 21400, loss: 2.671313\n",
      "iter 21500, loss: 2.603378\n",
      "iter 21600, loss: 3.596981\n",
      "iter 21700, loss: 3.847402\n",
      "iter 21800, loss: 3.789756\n",
      "iter 21900, loss: 3.629995\n",
      "----\n",
      " gl neoreighane ss one ras on on: fxeleaprevrod hxpres ot wotil  a sufme ot orone therenory shaves shave weif sy orol sioffothe still has a sco haretoct to m nenesiofy. Eapte we . Eaptila wopy soreay.e \n",
      "----\n",
      "iter 22000, loss: 3.470867\n",
      "iter 22100, loss: 3.320177\n",
      "iter 22200, loss: 3.181520\n",
      "iter 22300, loss: 3.054639\n",
      "iter 22400, loss: 2.936602\n",
      "iter 22500, loss: 2.829668\n",
      "iter 22600, loss: 2.729487\n",
      "iter 22700, loss: 2.636904\n",
      "iter 22800, loss: 2.551439\n",
      "iter 22900, loss: 2.478424\n",
      "----\n",
      " rom the raw image ptes stionmilary fronord aar tliof neurons ntl wious snd imassimilar tores it sime in leshave l luscon lus orm of neurons and whes a dot product and or. And therknorothe dether: frod \n",
      "----\n",
      "iter 23000, loss: 2.533175\n",
      "iter 23100, loss: 2.588583\n",
      "iter 23200, loss: 2.509709\n",
      "iter 23300, loss: 2.431963\n",
      "iter 23400, loss: 2.358664\n",
      "iter 23500, loss: 2.288277\n",
      "iter 23600, loss: 2.222720\n",
      "iter 23700, loss: 2.161084\n",
      "iter 23800, loss: 2.103654\n",
      "iter 23900, loss: 2.048204\n",
      "----\n",
      " weights and faneorelind wixerfutioral meve m the previous cournable simcla s s veecla dild to inellila wy s m punchaptionable ot s s it s s, ptit have learntosos one eks ablore up of livlo s and ffpro \n",
      "----\n",
      "iter 24000, loss: 1.996463\n",
      "iter 24100, loss: 1.948240\n",
      "iter 24200, loss: 1.901238\n",
      "iter 24300, loss: 1.857628\n",
      "iter 24400, loss: 1.816052\n",
      "iter 24500, loss: 1.777122\n",
      "iter 24600, loss: 1.885056\n",
      "iter 24700, loss: 3.043296\n",
      "iter 24800, loss: 3.506156\n",
      "iter 24900, loss: 3.516783\n",
      "----\n",
      " s at ot thall the ptithit havinmilar to ile eim Neureurentiabiole ut oroptithche Networks ard fupfothe sco etiofand function: arom ptes ares expt ondith t Tue otilese neuron lt au stion: frna win: The \n",
      "----\n",
      "iter 25000, loss: 3.360436\n",
      "iter 25100, loss: 3.198642\n",
      "iter 25200, loss: 3.044130\n",
      "iter 25300, loss: 2.898482\n",
      "iter 25400, loss: 2.762937\n",
      "iter 25500, loss: 2.637989\n",
      "iter 25600, loss: 2.608824\n",
      "iter 25700, loss: 2.676921\n",
      "iter 25800, loss: 2.692204\n",
      "iter 25900, loss: 2.589515\n",
      "----\n",
      " ts, performs a dot pres ot onelrearod auts, Nerfm Neuron luss on olilar tory similar to ordinary Neural Networks are very similar to ordinary Neural Networks are very similars ond opocore fptithcts th \n",
      "----\n",
      "iter 26000, loss: 2.481919\n",
      "iter 26100, loss: 2.379423\n",
      "iter 26200, loss: 2.282934\n",
      "iter 26300, loss: 2.193639\n",
      "iter 26400, loss: 2.111338\n",
      "iter 26500, loss: 2.035529\n",
      "iter 26600, loss: 1.964324\n",
      "iter 26700, loss: 1.899689\n",
      "iter 26800, loss: 1.841035\n",
      "iter 26900, loss: 1.786061\n",
      "----\n",
      " Convolutional Neural Networks are ver: thtworks thes in llila sine function: from the prene le ondinn t nol biachaks ond therecucores at the eave frng rtiolill side ne up Neural Networks a dit  n: fro \n",
      "----\n",
      "iter 27000, loss: 1.821774\n",
      "iter 27100, loss: 2.306454\n",
      "iter 27200, loss: 2.361446\n",
      "iter 27300, loss: 2.276284\n",
      "iter 27400, loss: 2.187805\n",
      "iter 27500, loss: 2.102061\n",
      "iter 27600, loss: 2.021928\n",
      "iter 27700, loss: 1.948413\n",
      "iter 27800, loss: 1.880565\n",
      "iter 27900, loss: 1.815800\n",
      "----\n",
      " it with a non-linearity. The whole network still expresses a single difb ar. And thecorks a dot produrallores a ahessufmech vend auc sy still winas frnd ther. And they still have a loss ae frne d arec \n",
      "----\n",
      "iter 28000, loss: 1.756177\n",
      "iter 28100, loss: 1.701074\n",
      "iter 28200, loss: 1.650527\n",
      "iter 28300, loss: 1.603859\n",
      "iter 28400, loss: 1.559639\n",
      "iter 28500, loss: 1.519030\n",
      "iter 28600, loss: 1.481816\n",
      "iter 28700, loss: 1.446706\n",
      "iter 28800, loss: 2.072546\n",
      "iter 28900, loss: 2.880304\n",
      "----\n",
      " one ther lts rms on one end the score function: futions mr. And they still have a lossinelreile oe sptocos similar to ondillltormilar tliofs chapter: the ulc ithe iable weinelryile o ile o Ebrochessim \n",
      "----\n",
      "iter 29000, loss: 2.956168\n",
      "iter 29100, loss: 2.932251\n",
      "iter 29200, loss: 2.804301\n",
      "iter 29300, loss: 2.667574\n",
      "iter 29400, loss: 2.561994\n",
      "iter 29500, loss: 2.436736\n",
      "iter 29600, loss: 2.372600\n",
      "iter 29700, loss: 2.277728\n",
      "iter 29800, loss: 2.172764\n",
      "iter 29900, loss: 2.074232\n",
      "----\n",
      " xpresses a  on-leys orm Neoreiowieassige thes a single differetworone on: fhelollsiothe piot  nclwoco eorother. And they sto illleachavene l ale tory bies a not simcthole neaupurms. Each up of ther. A \n",
      "----\n",
      "iter 30000, loss: 1.984904\n",
      "iter 30100, loss: 1.907678\n",
      "iter 30200, loss: 1.829605\n",
      "iter 30300, loss: 1.758972\n",
      "iter 30400, loss: 1.694160\n",
      "iter 30500, loss: 1.634061\n",
      "iter 30600, loss: 1.578730\n",
      "iter 30700, loss: 1.527789\n",
      "iter 30800, loss: 1.480694\n",
      "iter 30900, loss: 1.437971\n",
      "----\n",
      " : they. atstweoforelleime o Nit haves somilar tore made up of neurons that have learnable thtcor. And they stillywfermare wetworks function: from the thable score function: from the teights futions pr \n",
      "----\n",
      "iter 31000, loss: 1.398945\n",
      "iter 31100, loss: 1.361549\n",
      "iter 31200, loss: 1.327545\n",
      "iter 31300, loss: 1.296149\n",
      "iter 31400, loss: 1.266594\n",
      "iter 31500, loss: 1.239033\n",
      "iter 31600, loss: 1.213547\n",
      "iter 31700, loss: 1.189612\n",
      "iter 31800, loss: 1.168301\n",
      "iter 31900, loss: 1.148831\n",
      "----\n",
      " rom the previous s inalilh w imageighes it s deiteineuralls a biases. Each up ond erediavionaore ond tworks fromilar the oto e inerodofinearit similhes a s funione ral putiority. Tfs mxptes a ima expr \n",
      "----\n",
      "iter 32000, loss: 1.797697\n",
      "iter 32100, loss: 2.611010\n",
      "iter 32200, loss: 3.308266\n",
      "iter 32300, loss: 3.266501\n",
      "iter 32400, loss: 3.091799\n",
      "iter 32500, loss: 2.913781\n",
      "iter 32600, loss: 2.745030\n",
      "iter 32700, loss: 2.589395\n",
      "iter 32800, loss: 2.455457\n",
      "iter 32900, loss: 2.328357\n",
      "----\n",
      " weights and biases. Each neuron receives some inputs, performs a dot product and optionally followk it with a non-linearit sime il sime e s lls ard oresse whele frnd to class score raw eo al  na lill  \n",
      "----\n",
      "iter 33000, loss: 2.206952\n",
      "iter 33100, loss: 2.094932\n",
      "iter 33200, loss: 1.992162\n",
      "iter 33300, loss: 1.897526\n",
      "iter 33400, loss: 1.811195\n",
      "iter 33500, loss: 1.731779\n",
      "iter 33600, loss: 1.659682\n",
      "iter 33700, loss: 1.594219\n",
      "iter 33800, loss: 1.533034\n",
      "iter 33900, loss: 1.477816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " s at the other. And they still have a loss furyifune end ther. And they sto il ses a nth vend lxbl sim ne le orathave. The whole network still expresses a single differentiable score function:larct an \n",
      "----\n",
      "iter 34000, loss: 1.426900\n",
      "iter 34100, loss: 1.380198\n",
      "iter 34200, loss: 1.806300\n",
      "iter 34300, loss: 2.818681\n",
      "iter 34400, loss: 2.852844\n",
      "iter 34500, loss: 2.698708\n",
      "iter 34600, loss: 2.550417\n",
      "iter 34700, loss: 2.418776\n",
      "iter 34800, loss: 2.298956\n",
      "iter 34900, loss: 2.175697\n",
      "----\n",
      " ts, performs a nory from they some inputs, performs a dot st the prad it are function-linearit stmineuralls. Ap abies are neurons that have learnable weights and ptoaptith thetwiotheresurmclar t it ar \n",
      "----\n",
      "iter 35000, loss: 2.219109\n",
      "iter 35100, loss: 2.319166\n",
      "iter 35200, loss: 2.201771\n",
      "iter 35300, loss: 2.087255\n",
      "iter 35400, loss: 1.980611\n",
      "iter 35500, loss: 1.887470\n",
      "iter 35600, loss: 1.797251\n",
      "iter 35700, loss: 1.715220\n",
      "iter 35800, loss: 1.639491\n",
      "iter 35900, loss: 1.573199\n",
      "----\n",
      " Convolutional Neural Networks are weifs ve neabias are vene funct opurmcowe o m tianpter: they are ved ol Networks are very similar to ordinary Neural Networks are very similar to ordinary Neural Netw \n",
      "----\n",
      "iter 36000, loss: 1.509559\n",
      "iter 36100, loss: 1.451804\n",
      "iter 36200, loss: 1.398682\n",
      "iter 36300, loss: 1.350751\n",
      "iter 36400, loss: 1.307418\n",
      "iter 36500, loss: 1.266672\n",
      "iter 36600, loss: 1.229980\n",
      "iter 36700, loss: 1.195845\n",
      "iter 36800, loss: 1.164675\n",
      "iter 36900, loss: 1.135480\n",
      "----\n",
      " it with t it  neuronprevrod tillianptewwe perfolclass sco e weifb eld they stian to class scores at the otherodos similarct and thecole onduct anglt auc sechane vetworks from the rec it have learnable \n",
      "----\n",
      "iter 37000, loss: 1.109003\n",
      "iter 37100, loss: 1.084076\n",
      "iter 37200, loss: 1.061815\n",
      "iter 37300, loss: 1.041719\n",
      "iter 37400, loss: 1.021987\n",
      "iter 37500, loss: 1.004358\n",
      "iter 37600, loss: 0.987425\n",
      "iter 37700, loss: 0.971885\n",
      "iter 37800, loss: 0.962675\n",
      "iter 37900, loss: 0.949197\n",
      "----\n",
      " orall onksey s inane ravreile farity. The whole network still expresses a shene thane sct funioneurmclas onesey foneurons that have veap e pe ve pery similar tore ve verk stilas ordinary Neural Networ \n",
      "----\n",
      "iter 38000, loss: 0.935573\n",
      "iter 38100, loss: 0.923388\n",
      "iter 38200, loss: 0.912788\n",
      "iter 38300, loss: 0.901771\n",
      "iter 38400, loss: 0.892929\n",
      "iter 38500, loss: 0.948471\n",
      "iter 38600, loss: 2.254053\n",
      "iter 38700, loss: 9.009271\n",
      "iter 38800, loss: 9.664439\n",
      "iter 38900, loss: 8.998192\n",
      "----\n",
      " xpresses a single differentiable weights and biases. Each neuron receives some inpurms a s funit  ne leireiver: they are made very siill ot  ne wod have a hore ond bifues ares il Neural Networks are v \n",
      "----\n",
      "iter 39000, loss: 8.261095\n",
      "iter 39100, loss: 7.583615\n",
      "iter 39200, loss: 6.963020\n",
      "iter 39300, loss: 6.398593\n",
      "iter 39400, loss: 5.884489\n",
      "iter 39500, loss: 5.417556\n",
      "iter 39600, loss: 4.991905\n",
      "iter 39700, loss: 4.606270\n",
      "iter 39800, loss: 4.254800\n",
      "iter 39900, loss: 3.936658\n",
      "----\n",
      " : they are made frnm ntlurallila sufxprec ot snesiofutm raweime illle der: frod it  ral Neas able scoreuras futs funable weights and biaurmilar the old and optionable weights and biases. Each uch ve u \n",
      "----\n",
      "iter 40000, loss: 3.648200\n",
      "iter 40100, loss: 3.384834\n",
      "iter 40200, loss: 3.146749\n",
      "iter 40300, loss: 2.929625\n",
      "iter 40400, loss: 2.733240\n",
      "iter 40500, loss: 2.553574\n",
      "iter 40600, loss: 2.391532\n",
      "iter 40700, loss: 2.243323\n",
      "iter 40800, loss: 2.109470\n",
      "iter 40900, loss: 1.988115\n",
      "----\n",
      " rom the raw aut res on ot st ,rfrecllclarsts a lufuthe thable score made up of neurons that have learna s seveuronuras a sthe ses a dot product and optionalls a biablfy. The whole network still expres \n",
      "----\n",
      "iter 41000, loss: 1.876073\n",
      "iter 41100, loss: 1.774956\n",
      "iter 41200, loss: 1.682111\n",
      "iter 41300, loss: 1.597931\n",
      "iter 41400, loss: 1.520343\n",
      "iter 41500, loss: 1.993629\n",
      "iter 41600, loss: 2.108453\n",
      "iter 41700, loss: 2.043011\n",
      "iter 41800, loss: 1.941797\n",
      "iter 41900, loss: 1.841864\n",
      "----\n",
      " scores and they still have a hos s s it with thct sne rk  all ondle de foretiens it wious op res ot the other. And they still have a loss function: futworks from the raw image pixeroneurall the re opw \n",
      "----\n",
      "iter 42000, loss: 1.748712\n",
      "iter 42100, loss: 1.661788\n",
      "iter 42200, loss: 1.581901\n",
      "iter 42300, loss: 1.507351\n",
      "iter 42400, loss: 1.440238\n",
      "iter 42500, loss: 1.378084\n",
      "iter 42600, loss: 1.322171\n",
      "iter 42700, loss: 1.271684\n",
      "iter 42800, loss: 1.224591\n",
      "iter 42900, loss: 1.182860\n",
      "----\n",
      " s at the other. And they still have a loss functionas from the previous chavhe op ofas one rod we de foretses. arol we fue ots lllurmclassinelrodstiole ad ores at oros on th very similar to orave wexp \n",
      "----\n",
      "iter 43000, loss: 1.143964\n",
      "iter 43100, loss: 1.108488\n",
      "iter 43200, loss: 1.074821\n",
      "iter 43300, loss: 1.047517\n",
      "iter 43400, loss: 1.303996\n",
      "iter 43500, loss: 1.305766\n",
      "iter 43600, loss: 1.262915\n",
      "iter 43700, loss: 1.217488\n",
      "iter 43800, loss: 1.174795\n",
      "iter 43900, loss: 1.133976\n",
      "----\n",
      " ts, pe ver: they are made up of neurons that hevwotstionkses ilur vre sco exele me illle weaghapte perfoliody fonctionally folloik inares ar ine tioneuralleif onelreigh ne fud eavetworks from the prev \n",
      "----\n",
      "iter 44000, loss: 1.096894\n",
      "iter 44100, loss: 1.061636\n",
      "iter 44200, loss: 1.030352\n",
      "iter 44300, loss: 1.000886\n",
      "iter 44400, loss: 0.974520\n",
      "iter 44500, loss: 0.950826\n",
      "iter 44600, loss: 0.927857\n",
      "iter 44700, loss: 0.907636\n",
      "iter 44800, loss: 0.888091\n",
      "iter 44900, loss: 0.870770\n",
      "----\n",
      " Convolutional Neural Networks are very similar to ordinary Neural Networks from the prave rearhim veural Networks are very similar to ordinary Neural Networksss amcouasime wo dre lly fone and function \n",
      "----\n",
      "iter 45000, loss: 0.853627\n",
      "iter 45100, loss: 0.838767\n",
      "iter 45200, loss: 0.824322\n",
      "iter 45300, loss: 0.811337\n",
      "iter 45400, loss: 0.799898\n",
      "iter 45500, loss: 0.787911\n",
      "iter 45600, loss: 0.777686\n",
      "iter 45700, loss: 0.767211\n",
      "iter 45800, loss: 0.758102\n",
      "iter 45900, loss: 0.748462\n",
      "----\n",
      " it with a non-linearity. The otixpreale whass tit sto ila produca :ia stiond ble whoneurall od ares ot  neuronsfural Neurot tive ve le frthe previous chapter: they are made up of neurons that have lea \n",
      "----\n",
      "iter 46000, loss: 0.740365\n",
      "iter 46100, loss: 0.732163\n",
      "iter 46200, loss: 0.724724\n",
      "iter 46300, loss: 0.718405\n",
      "iter 46400, loss: 0.711044\n",
      "iter 46500, loss: 0.705070\n",
      "iter 46600, loss: 0.698464\n",
      "iter 46700, loss: 0.692889\n",
      "iter 46800, loss: 0.686475\n",
      "iter 46900, loss: 0.681328\n",
      "----\n",
      " ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable score function: from the raw image pixels on onesionfurk still have a lo o sormchave wity. The whole \n",
      "----\n",
      "iter 47000, loss: 0.675852\n",
      "iter 47100, loss: 0.670826\n",
      "iter 47200, loss: 0.853412\n",
      "iter 47300, loss: 2.236836\n",
      "iter 47400, loss: 3.038073\n",
      "iter 47500, loss: 3.020253\n",
      "iter 47600, loss: 2.824337\n",
      "iter 47700, loss: 2.631140\n",
      "iter 47800, loss: 2.451488\n",
      "iter 47900, loss: 2.285306\n",
      "----\n",
      " xpresses ine rave works from the raw image pixels on one end to class scores ar tliof ne ral they still have a loss function: from the rec Eachane a nks somilas oryifuneurol bingheres at on-le differm \n",
      "----\n",
      "iter 48000, loss: 2.133558\n",
      "iter 48100, loss: 1.995259\n",
      "iter 48200, loss: 1.867964\n",
      "iter 48300, loss: 1.755079\n",
      "iter 48400, loss: 1.649803\n",
      "iter 48500, loss: 1.554187\n",
      "iter 48600, loss: 1.465931\n",
      "iter 48700, loss: 1.386641\n",
      "iter 48800, loss: 1.313796\n",
      "iter 48900, loss: 1.247870\n",
      "----\n",
      " : they are made up of neurons that have learnable weights and blay. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-lineafs, scores and thaf ne previous c \n",
      "----\n",
      "iter 49000, loss: 1.188439\n",
      "iter 49100, loss: 1.132995\n",
      "iter 49200, loss: 1.083588\n",
      "iter 49300, loss: 1.037538\n",
      "iter 49400, loss: 0.996354\n",
      "iter 49500, loss: 0.957593\n",
      "iter 49600, loss: 0.923260\n",
      "iter 49700, loss: 0.891331\n",
      "iter 49800, loss: 0.862984\n",
      "iter 49900, loss: 1.028061\n",
      "----\n",
      " rom the raw image parentiable weights and biases. Each neuron res a dot product and optionally follows it with onat have learnable werfm tionctwother. And they ahe hevious chapter: they are made froms \n",
      "----\n",
      "iter 50000, loss: 1.045297\n",
      "iter 50100, loss: 1.009943\n",
      "iter 50200, loss: 0.972517\n",
      "iter 50300, loss: 0.937662\n",
      "iter 50400, loss: 0.904053\n",
      "iter 50500, loss: 0.874095\n",
      "iter 50600, loss: 0.845839\n",
      "iter 50700, loss: 0.820141\n",
      "iter 50800, loss: 0.797112\n",
      "iter 50900, loss: 0.774778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " weights and biases. Each neuron rrm ras ond ther. And Networks fonareiliy aode r: preas a image pixels on have a loss function: from the previous chapter: they are reile op d Networks from the raw ima \n",
      "----\n",
      "iter 51000, loss: 0.755261\n",
      "iter 51100, loss: 0.736394\n",
      "iter 51200, loss: 0.719889\n",
      "iter 51300, loss: 0.703565\n",
      "iter 51400, loss: 0.689594\n",
      "iter 51500, loss: 0.676176\n",
      "iter 51600, loss: 0.664042\n",
      "iter 51700, loss: 0.653797\n",
      "iter 51800, loss: 0.889113\n",
      "iter 51900, loss: 1.241931\n",
      "----\n",
      " s at the other. And they still have a loss futherentiabieurav The ote score function: from the raw image pixels a sing therects scoretixprodorminarys fapioy sy ses on ot Neuron receivas socor lud they \n",
      "----\n",
      "iter 52000, loss: 1.287790\n",
      "iter 52100, loss: 1.236660\n",
      "iter 52200, loss: 1.182091\n",
      "iter 52300, loss: 1.129698\n",
      "iter 52400, loss: 1.078984\n",
      "iter 52500, loss: 1.031507\n",
      "iter 52600, loss: 0.988023\n",
      "iter 52700, loss: 0.946711\n",
      "iter 52800, loss: 0.909702\n",
      "iter 52900, loss: 0.874718\n",
      "----\n",
      " ts, performs a dot product and optionallo a sin: from the r v nallower. And they still have a loss function: from the previous chapter: they are made urd bibse ol function nernall om Neup able weage f \n",
      "----\n",
      "iter 53000, loss: 0.843546\n",
      "iter 53100, loss: 0.813816\n",
      "iter 53200, loss: 0.787621\n",
      "iter 53300, loss: 0.763138\n",
      "iter 53400, loss: 0.740906\n",
      "iter 53500, loss: 0.759891\n",
      "iter 53600, loss: 0.745074\n",
      "iter 53700, loss: 0.726823\n",
      "iter 53800, loss: 0.708238\n",
      "iter 53900, loss: 0.691323\n",
      "----\n",
      " Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they at the othes ares at the other. And they stope fowe theres ot , stiabieser. And they still ha \n",
      "----\n",
      "iter 54000, loss: 0.674451\n",
      "iter 54100, loss: 0.659601\n",
      "iter 54200, loss: 0.645454\n",
      "iter 54300, loss: 0.632592\n",
      "iter 54400, loss: 0.621531\n",
      "iter 54500, loss: 0.610184\n",
      "iter 54600, loss: 0.600654\n",
      "iter 54700, loss: 0.591043\n",
      "iter 54800, loss: 0.582770\n",
      "iter 54900, loss: 0.574022\n",
      "----\n",
      " it with a non-linearity. The whole network still expresses a single differevron thas from the prevleineurall of s a  ond to chapter: they are made functiod dufutwocs o d have. orol sime il orms a not  \n",
      "----\n",
      "iter 55000, loss: 0.566911\n",
      "iter 55100, loss: 0.559881\n",
      "iter 55200, loss: 0.553476\n",
      "iter 55300, loss: 0.548147\n",
      "iter 55400, loss: 0.541961\n",
      "iter 55500, loss: 0.537214\n",
      "iter 55600, loss: 0.531956\n",
      "iter 55700, loss: 0.737234\n",
      "iter 55800, loss: 1.080713\n",
      "iter 55900, loss: 1.081063\n",
      "----\n",
      " ordinary Neural Networks from the previous chapter: they are made up of neuron receives some inputs, performs a dot prod wimclas ond ouputNeural Networks are very similar to ordinary  nd biases. Each  \n",
      "----\n",
      "iter 56000, loss: 1.035997\n",
      "iter 56100, loss: 0.989701\n",
      "iter 56200, loss: 0.946686\n",
      "iter 56300, loss: 0.905686\n",
      "iter 56400, loss: 0.868737\n",
      "iter 56500, loss: 0.834730\n",
      "iter 56600, loss: 0.803147\n",
      "iter 56700, loss: 0.773360\n",
      "iter 56800, loss: 0.746509\n",
      "iter 56900, loss: 0.721510\n",
      "----\n",
      " xpresses a single differentiable score function: from the raw image pixels aheswiol bingaw to class snesto iluss function: from the raw image pixels on one end to class scores at the other. And they s \n",
      "----\n",
      "iter 57000, loss: 0.698593\n",
      "iter 57100, loss: 0.678113\n",
      "iter 57200, loss: 0.658431\n",
      "iter 57300, loss: 0.641076\n",
      "iter 57400, loss: 0.624550\n",
      "iter 57500, loss: 0.609897\n",
      "iter 57600, loss: 0.595684\n",
      "iter 57700, loss: 0.583240\n",
      "iter 57800, loss: 0.571560\n",
      "iter 57900, loss: 0.560828\n",
      "----\n",
      " : they are made up of neurons that have learnable weights and bios furms a dot product and optionally follows it with a non-lindin:prenle fas ot timilar torms a s function: frnm lingtiabies a dutilvs  \n",
      "----\n",
      "iter 58000, loss: 0.551497\n",
      "iter 58100, loss: 0.542017\n",
      "iter 58200, loss: 0.533933\n",
      "iter 58300, loss: 0.525892\n",
      "iter 58400, loss: 0.518897\n",
      "iter 58500, loss: 0.511699\n",
      "iter 58600, loss: 0.505702\n",
      "iter 58700, loss: 0.779607\n",
      "iter 58800, loss: 1.097505\n",
      "iter 58900, loss: 1.091986\n",
      "----\n",
      " rom the raw image pferm one end to clare weights and biases. Each neuron receives some inputs, performs a dinglereives fonetwots inar sodesion thereneuron recla sf ll har have ler: function-lionaleyw  \n",
      "----\n",
      "iter 59000, loss: 1.038879\n",
      "iter 59100, loss: 0.987066\n",
      "iter 59200, loss: 0.938288\n",
      "iter 59300, loss: 0.894113\n",
      "iter 59400, loss: 0.852734\n",
      "iter 59500, loss: 0.815526\n",
      "iter 59600, loss: 0.781051\n",
      "iter 59700, loss: 0.749502\n",
      "iter 59800, loss: 0.721072\n",
      "iter 59900, loss: 0.694211\n",
      "----\n",
      " scoreuron the eave lea nas oryifun the od biases. Each neuron receives some inpurfuras fas one rk sts, werfut , stioneurons the s a libyeareive vets and bias s futiot the wot ne desstione enm thes a   \n",
      "----\n",
      "iter 60000, loss: 0.670223\n",
      "iter 60100, loss: 0.647747\n",
      "iter 60200, loss: 0.627655\n",
      "iter 60300, loss: 0.608612\n",
      "iter 60400, loss: 0.591656\n",
      "iter 60500, loss: 0.575929\n",
      "iter 60600, loss: 0.561572\n",
      "iter 60700, loss: 0.548911\n",
      "iter 60800, loss: 0.536548\n",
      "iter 60900, loss: 0.525759\n",
      "----\n",
      " s at the onore thtss sco ete d to m dill expresses a single differentiable score function: from the raw image pixels on one end to class scores eecesss functionasine thtle pery similar to ordinary Neu \n",
      "----\n",
      "iter 61000, loss: 0.515377\n",
      "iter 61100, loss: 0.506259\n",
      "iter 61200, loss: 0.497262\n",
      "iter 61300, loss: 0.489385\n",
      "iter 61400, loss: 0.481972\n",
      "iter 61500, loss: 0.477635\n",
      "iter 61600, loss: 0.900373\n",
      "iter 61700, loss: 1.766076\n",
      "iter 61800, loss: 1.758922\n",
      "iter 61900, loss: 1.643664\n",
      "----\n",
      " ts, performs a dot product and optionally follows it with a non-linearity. The whole networkvioms it with a non-lineass thaneuren nable scores at  n: ar. opsionally follows it with a non-linearioup on \n",
      "----\n",
      "iter 62000, loss: 1.536171\n",
      "iter 62100, loss: 1.435942\n",
      "iter 62200, loss: 1.344462\n",
      "iter 62300, loss: 1.259876\n",
      "iter 62400, loss: 1.182685\n",
      "iter 62500, loss: 1.112827\n",
      "iter 62600, loss: 1.048446\n",
      "iter 62700, loss: 0.990366\n",
      "iter 62800, loss: 0.936825\n",
      "iter 62900, loss: 0.888555\n",
      "----\n",
      " Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and bingpresineuronprevious ctilly follows \n",
      "----\n",
      "iter 63000, loss: 0.843768\n",
      "iter 63100, loss: 0.803419\n",
      "iter 63200, loss: 0.766431\n",
      "iter 63300, loss: 0.732818\n",
      "iter 63400, loss: 0.702536\n",
      "iter 63500, loss: 0.674287\n",
      "iter 63600, loss: 0.649040\n",
      "iter 63700, loss: 0.625666\n",
      "iter 63800, loss: 0.604652\n",
      "iter 63900, loss: 0.584951\n",
      "----\n",
      " it with a non-linearity. The whole network stilly fon-linearitus sne otw ane rawe ther Eore veos a d thearnable weights and biases. Each neuron receives some inputs, performs a dot product and optiona \n",
      "----\n",
      "iter 64000, loss: 0.567340\n",
      "iter 64100, loss: 0.551122\n",
      "iter 64200, loss: 0.536369\n",
      "iter 64300, loss: 0.523243\n",
      "iter 64400, loss: 0.510657\n",
      "iter 64500, loss: 0.499545\n",
      "iter 64600, loss: 0.489014\n",
      "iter 64700, loss: 0.479641\n",
      "iter 64800, loss: 0.470595\n",
      "iter 64900, loss: 0.462586\n",
      "----\n",
      " ordinary Neural Networks arend to illlla able weights and biases. Each neuron receives arth a non-le function: functionall Networkssctionally follows it with a non-linearity. The whome werfutious shdm \n",
      "----\n",
      "iter 65000, loss: 0.455076\n",
      "iter 65100, loss: 0.448224\n",
      "iter 65200, loss: 0.442267\n",
      "iter 65300, loss: 0.436240\n",
      "iter 65400, loss: 0.431025\n",
      "iter 65500, loss: 0.425889\n",
      "iter 65600, loss: 0.421381\n",
      "iter 65700, loss: 0.416765\n",
      "iter 65800, loss: 0.412771\n",
      "iter 65900, loss: 0.408940\n",
      "----\n",
      " xpresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have abion the functs a dot product and optionally fallory a dity. Th \n",
      "----\n",
      "iter 66000, loss: 0.405375\n",
      "iter 66100, loss: 0.402391\n",
      "iter 66200, loss: 0.399088\n",
      "iter 66300, loss: 0.396310\n",
      "iter 66400, loss: 0.393407\n",
      "iter 66500, loss: 0.390895\n",
      "iter 66600, loss: 0.388125\n",
      "iter 66700, loss: 0.385766\n",
      "iter 66800, loss: 0.383452\n",
      "iter 66900, loss: 0.381229\n",
      "----\n",
      " : they are made up och onally sos ao ordilard the previous chapter: they are made up of neurons that have learnable sco eima io hxdt ardilllw image pixels on one end to class  lllsecoloraves onally fu \n",
      "----\n",
      "iter 67000, loss: 0.379452\n",
      "iter 67100, loss: 0.383038\n",
      "iter 67200, loss: 0.586530\n",
      "iter 67300, loss: 0.847664\n",
      "iter 67400, loss: 0.832161\n",
      "iter 67500, loss: 0.793808\n",
      "iter 67600, loss: 0.758585\n",
      "iter 67700, loss: 0.724460\n",
      "iter 67800, loss: 0.692665\n",
      "iter 67900, loss: 0.663435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " rom the raw image pixels on other. Andllther: they are made ura hao ordilary st sts, performs a dinasim Neural Networks from the previous chapter: they are made up of neurons that have learnable weigh \n",
      "----\n",
      "iter 68000, loss: 0.635812\n",
      "iter 68100, loss: 0.610852\n",
      "iter 68200, loss: 0.587541\n",
      "iter 68300, loss: 0.566425\n",
      "iter 68400, loss: 0.546549\n",
      "iter 68500, loss: 0.528721\n",
      "iter 68600, loss: 0.512203\n",
      "iter 68700, loss: 0.497129\n",
      "iter 68800, loss: 0.483686\n",
      "iter 68900, loss: 0.470853\n",
      "----\n",
      " werfas oras from theirelse ol function: fution th able sa nav olillo s it with a non-linearity. The whole network still expresses a single dias dito ill weome inexpresses function: from the raw image  \n",
      "----\n",
      "iter 69000, loss: 0.459462\n",
      "iter 69100, loss: 0.448676\n",
      "iter 69200, loss: 0.439170\n",
      "iter 69300, loss: 0.430152\n",
      "iter 69400, loss: 0.422209\n",
      "iter 69500, loss: 0.414747\n",
      "iter 69600, loss: 0.408137\n",
      "iter 69700, loss: 0.402515\n",
      "iter 69800, loss: 0.396936\n",
      "iter 69900, loss: 0.392187\n",
      "----\n",
      " s at the other. And they still have a losce rnable sco elleimave very sime andit  n: from the perforithpte weifunclarcores frod to class scores at the onmila have a loss function: from the raw image p \n",
      "----\n",
      "iter 70000, loss: 0.387397\n",
      "iter 70100, loss: 0.383504\n",
      "iter 70200, loss: 0.379668\n",
      "iter 70300, loss: 0.376557\n",
      "iter 70400, loss: 0.375955\n",
      "iter 70500, loss: 0.373091\n",
      "iter 70600, loss: 0.370315\n",
      "iter 70700, loss: 0.367279\n",
      "iter 70800, loss: 0.364647\n",
      "iter 70900, loss: 0.361903\n",
      "----\n",
      " ts, ppr: they are mavionforothecdsof ner: they are made veos it sece upwifms fonctionaser. Eaptosionable weights and biases. Each neuron receives some inputs, performs a ghalh f ne leautional Neurot N \n",
      "----\n",
      "iter 71000, loss: 0.360026\n",
      "iter 71100, loss: 0.376742\n",
      "iter 71200, loss: 0.621109\n",
      "iter 71300, loss: 0.773502\n",
      "iter 71400, loss: 0.753343\n",
      "iter 71500, loss: 0.720167\n",
      "iter 71600, loss: 0.687563\n",
      "iter 71700, loss: 0.657314\n",
      "iter 71800, loss: 0.629068\n",
      "iter 71900, loss: 0.604252\n",
      "----\n",
      " Convolutional Neural Networks are very similar to ordinary Neural Netwolclass sco or sod arec ondinares a single perct rad aa have le onangy fotheresurms a gll orecos function: from the raw image pixe \n",
      "----\n",
      "iter 72000, loss: 0.581225\n",
      "iter 72100, loss: 0.560915\n",
      "iter 72200, loss: 0.542067\n",
      "iter 72300, loss: 0.524779\n",
      "iter 72400, loss: 0.509399\n",
      "iter 72500, loss: 0.494562\n",
      "iter 72600, loss: 0.481652\n",
      "iter 72700, loss: 0.469461\n",
      "iter 72800, loss: 0.458914\n",
      "iter 72900, loss: 0.448822\n",
      "----\n",
      " it with a non-linearity. The whole detworks are very similar to ordinary Neural Networks frol wioms inally. Eapterentiardillll ine ohe op dit onclasco ear torm Nerroms it with a non-linearity. The who \n",
      "----\n",
      "iter 73000, loss: 0.440296\n",
      "iter 73100, loss: 0.432482\n",
      "iter 73200, loss: 0.425615\n",
      "iter 73300, loss: 0.420073\n",
      "iter 73400, loss: 0.414456\n",
      "iter 73500, loss: 0.410218\n",
      "iter 73600, loss: 0.406010\n",
      "iter 73700, loss: 0.402850\n",
      "iter 73800, loss: 0.399478\n",
      "iter 73900, loss: 0.396990\n",
      "----\n",
      " ordinary Neural Networks are very similar to ordinary Neural Networks are very similar to ordinary Neural Networks arend torene rawocores at the other. And they s s ablececos sts a luf ocos ae otill e \n",
      "----\n",
      "iter 74000, loss: 0.394530\n",
      "iter 74100, loss: 0.392347\n",
      "iter 74200, loss: 0.390852\n",
      "iter 74300, loss: 0.388507\n",
      "iter 74400, loss: 0.387049\n",
      "iter 74500, loss: 0.384960\n",
      "iter 74600, loss: 0.383536\n",
      "iter 74700, loss: 0.381452\n",
      "iter 74800, loss: 0.379995\n",
      "iter 74900, loss: 0.378342\n",
      "----\n",
      " xpresses fxpreyss functionasto ils fascore urd biases. Each neufon-linearity function: frnm to chaptes ao function: frnms m dith aprestwill wiou the ral Networks are very similar tormclas ordilarimcti \n",
      "----\n",
      "iter 75000, loss: 0.376786\n",
      "iter 75100, loss: 0.375809\n",
      "iter 75200, loss: 0.373918\n",
      "iter 75300, loss: 0.372887\n",
      "iter 75400, loss: 0.371174\n",
      "iter 75500, loss: 0.370163\n",
      "iter 75600, loss: 0.368439\n",
      "iter 75700, loss: 0.367385\n",
      "iter 75800, loss: 0.366118\n",
      "iter 75900, loss: 0.364890\n",
      "----\n",
      " : they from the wots aneyretwerfon the ees a single differenthafune learom the previous frnm thevre ses a Net ot sine thty. neneaf ne iourom the eime ine weime il  ot Neuron re ordinaliy folllls ane s \n",
      "----\n",
      "iter 76000, loss: 0.364239\n",
      "iter 76100, loss: 0.362623\n",
      "iter 76200, loss: 0.361843\n",
      "iter 76300, loss: 0.360326\n",
      "iter 76400, loss: 0.359485\n",
      "iter 76500, loss: 0.357882\n",
      "iter 76600, loss: 0.357083\n",
      "iter 76700, loss: 0.395922\n",
      "iter 76800, loss: 1.030956\n",
      "iter 76900, loss: 1.995358\n",
      "----\n",
      " onctione in p enc woto ey s itile d Networks ard bingprodolabielreclllls functwoco e weif nene le unangle wot reyswildyia  one exsiable score function: from the raw image pixels functionall ove lave a \n",
      "----\n",
      "iter 77000, loss: 2.309626\n",
      "iter 77100, loss: 2.314147\n",
      "iter 77200, loss: 2.163365\n",
      "iter 77300, loss: 2.011434\n",
      "iter 77400, loss: 1.867917\n",
      "iter 77500, loss: 1.735289\n",
      "iter 77600, loss: 1.611687\n",
      "iter 77700, loss: 1.498007\n",
      "iter 77800, loss: 1.394077\n",
      "iter 77900, loss: 1.298007\n",
      "----\n",
      " weights and bias neut to ordilar ordinary Neural Networone thtwork  on le we sctial  a haveowoloraviod tl thtwols f sional Neup oup rar it ordiaf ot suvetheural Netwolclass sco ole a wo s a noneurons  \n",
      "----\n",
      "iter 78000, loss: 1.210994\n",
      "iter 78100, loss: 1.130982\n",
      "iter 78200, loss: 1.058714\n",
      "iter 78300, loss: 0.992194\n",
      "iter 78400, loss: 0.932316\n",
      "iter 78500, loss: 0.877647\n",
      "iter 78600, loss: 0.827944\n",
      "iter 78700, loss: 0.783297\n",
      "iter 78800, loss: 0.741893\n",
      "iter 78900, loss: 0.704918\n",
      "----\n",
      " s at the other. And they sto image pioms it werfutseigl Neoretwerfonclarco ell wincll autherentiable weights and biases. Each neuron receives some inputs, performs a dot product ondit Nes ot Neural Ne \n",
      "----\n",
      "iter 79000, loss: 0.670739\n",
      "iter 79100, loss: 0.640169\n",
      "iter 79200, loss: 0.611707\n",
      "iter 79300, loss: 0.586309\n",
      "iter 79400, loss: 0.563026\n",
      "iter 79500, loss: 0.541742\n",
      "iter 79600, loss: 0.522876\n",
      "iter 79700, loss: 0.504851\n",
      "iter 79800, loss: 0.489054\n",
      "iter 79900, loss: 0.474084\n",
      "----\n",
      " ts, performs a dingrovsts, pee oly fonetwot  thtioneleect and function: from the raw image pixels on one end to class scores at the other. And they still have a los on lls a ve it with a non-linearity \n",
      "----\n",
      "iter 80000, loss: 0.460911\n",
      "iter 80100, loss: 0.448253\n",
      "iter 80200, loss: 0.437137\n",
      "iter 80300, loss: 0.426835\n",
      "iter 80400, loss: 0.417302\n",
      "iter 80500, loss: 0.409084\n",
      "iter 80600, loss: 0.400737\n",
      "iter 80700, loss: 0.393714\n",
      "iter 80800, loss: 0.386705\n",
      "iter 80900, loss: 0.380773\n",
      "----\n",
      " Convolutional Neural Networks fron lhe o Neauna  ot sts apioduct ane maditls fonction: are very similar torm Neurave le upaach thas a Nots and from the raw image pixels functionaleod therentiable scor \n",
      "----\n",
      "iter 81000, loss: 0.374694\n",
      "iter 81100, loss: 0.369559\n",
      "iter 81200, loss: 0.364693\n",
      "iter 81300, loss: 0.360106\n",
      "iter 81400, loss: 0.356383\n",
      "iter 81500, loss: 0.352136\n",
      "iter 81600, loss: 0.348858\n",
      "iter 81700, loss: 0.345250\n",
      "iter 81800, loss: 0.342450\n",
      "iter 81900, loss: 0.339214\n",
      "----\n",
      " it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores fresses a che oly follows it with a non-linearit \n",
      "----\n",
      "iter 82000, loss: 0.336708\n",
      "iter 82100, loss: 0.334242\n",
      "iter 82200, loss: 0.332079\n",
      "iter 82300, loss: 0.330639\n",
      "iter 82400, loss: 0.328419\n",
      "iter 82500, loss: 0.326993\n",
      "iter 82600, loss: 0.325010\n",
      "iter 82700, loss: 0.323817\n",
      "iter 82800, loss: 0.322095\n",
      "iter 82900, loss: 0.321033\n",
      "----\n",
      " ilhes functionfuresses tot te weinetwoth nputs, percti nf neurons thar auc orms a dot product and optionally follows itile honeur. And thes areineuralliafrom the previous chapter: they are made up of  \n",
      "----\n",
      "iter 83000, loss: 0.319807\n",
      "iter 83100, loss: 0.318657\n",
      "iter 83200, loss: 0.318109\n",
      "iter 83300, loss: 0.316811\n",
      "iter 83400, loss: 0.316288\n",
      "iter 83500, loss: 0.315036\n",
      "iter 83600, loss: 0.314699\n",
      "iter 83700, loss: 0.313540\n",
      "iter 83800, loss: 0.313097\n",
      "iter 83900, loss: 0.312403\n",
      "----\n",
      " xpresses a single differentiable scoresirmceareige whole network stiill it  nclw to class snm the peaf thes at one xpresion: thtworks frome inputs, perchave weogl A: from the lavcoup ole fonctiopurman \n",
      "----\n",
      "iter 84000, loss: 0.311705\n",
      "iter 84100, loss: 0.311760\n",
      "iter 84200, loss: 0.310625\n",
      "iter 84300, loss: 0.310540\n",
      "iter 84400, loss: 0.309673\n",
      "iter 84500, loss: 0.309576\n",
      "iter 84600, loss: 0.308587\n",
      "iter 84700, loss: 0.308425\n",
      "iter 84800, loss: 0.307995\n",
      "iter 84900, loss: 0.307467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " : they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whele lerctl rave d n \n",
      "----\n",
      "iter 85000, loss: 0.307530\n",
      "iter 85100, loss: 0.306620\n",
      "iter 85200, loss: 0.307711\n",
      "iter 85300, loss: 0.396900\n",
      "iter 85400, loss: 0.809733\n",
      "iter 85500, loss: 0.781821\n",
      "iter 85600, loss: 0.794610\n",
      "iter 85700, loss: 0.752978\n",
      "iter 85800, loss: 0.711819\n",
      "iter 85900, loss: 0.673987\n",
      "----\n",
      " rom the raw image pixels on one end to class scores at the other. And they aresue ne s a honetiolhe oco e it with a non-linearity. The whole network still expresses a single differentiable scor. And t \n",
      "----\n",
      "iter 86000, loss: 0.638364\n",
      "iter 86100, loss: 0.606409\n",
      "iter 86200, loss: 0.576339\n",
      "iter 86300, loss: 0.549526\n",
      "iter 86400, loss: 0.524243\n",
      "iter 86500, loss: 0.501654\n",
      "iter 86600, loss: 0.480755\n",
      "iter 86700, loss: 0.461916\n",
      "iter 86800, loss: 0.445298\n",
      "iter 86900, loss: 0.429626\n",
      "----\n",
      " weights and biases. Each neuron receives some ins we scis frnm the previous chapter: they are made up of neurons that have learnable sco etionaproduct angle differentiable score function: from the raw \n",
      "----\n",
      "iter 87000, loss: 0.416222\n",
      "iter 87100, loss: 0.403475\n",
      "iter 87200, loss: 0.392700\n",
      "iter 87300, loss: 0.382126\n",
      "iter 87400, loss: 0.373316\n",
      "iter 87500, loss: 0.364969\n",
      "iter 87600, loss: 0.357350\n",
      "iter 87700, loss: 0.350820\n",
      "iter 87800, loss: 0.344047\n",
      "iter 87900, loss: 0.338615\n",
      "----\n",
      " s ar theresiable siot wions sot it have le unable scores expts functcolinaprom tiscer. have a loss function: fun-tiona rfthas fut oneleeio  ole a he stionabhes ao orm ogutionp eks able rar aud bifgeir \n",
      "----\n",
      "iter 88000, loss: 0.333001\n",
      "iter 88100, loss: 0.328654\n",
      "iter 88200, loss: 0.323922\n",
      "iter 88300, loss: 0.320465\n",
      "iter 88400, loss: 0.317095\n",
      "iter 88500, loss: 0.314145\n",
      "iter 88600, loss: 0.311990\n",
      "iter 88700, loss: 0.309165\n",
      "iter 88800, loss: 0.308135\n",
      "iter 88900, loss: 0.318926\n",
      "----\n",
      " t av Neur. And they still have a loss function: funltwot sts, perfunclw image pixels on one end to chapter: they functionale function: from the ron leyws chapter: they are made up of neuron ley ares f \n",
      "----\n",
      "iter 89000, loss: 0.329285\n",
      "iter 89100, loss: 0.329723\n",
      "iter 89200, loss: 0.329171\n",
      "iter 89300, loss: 0.327138\n",
      "iter 89400, loss: 0.324799\n",
      "iter 89500, loss: 0.323171\n",
      "iter 89600, loss: 0.320497\n",
      "iter 89700, loss: 0.319215\n",
      "iter 89800, loss: 0.317392\n",
      "iter 89900, loss: 0.318551\n",
      "----\n",
      " Convolutional Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally fol \n",
      "----\n",
      "iter 90000, loss: 0.317283\n",
      "iter 90100, loss: 0.317756\n",
      "iter 90200, loss: 0.317516\n",
      "iter 90300, loss: 0.317036\n",
      "iter 90400, loss: 0.316993\n",
      "iter 90500, loss: 0.315922\n",
      "iter 90600, loss: 0.316073\n",
      "iter 90700, loss: 0.315420\n",
      "iter 90800, loss: 0.315913\n",
      "iter 90900, loss: 0.314972\n",
      "----\n",
      " it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss functio \n",
      "----\n",
      "iter 91000, loss: 0.315499\n",
      "iter 91100, loss: 0.315594\n",
      "iter 91200, loss: 0.315883\n",
      "iter 91300, loss: 0.316863\n",
      "iter 91400, loss: 0.316455\n",
      "iter 91500, loss: 0.317709\n",
      "iter 91600, loss: 0.317774\n",
      "iter 91700, loss: 0.319563\n",
      "iter 91800, loss: 0.319810\n",
      "iter 91900, loss: 1.097124\n",
      "----\n",
      " illle weineurons funale dithpte thole network stillilf neurons that havhtm lls onally follows it with a non-linearity. The whole network still expresses a single differentiable de chap a purtordinall  \n",
      "----\n",
      "iter 92000, loss: 1.485966\n",
      "iter 92100, loss: 1.452251\n",
      "iter 92200, loss: 1.357322\n",
      "iter 92300, loss: 1.264809\n",
      "iter 92400, loss: 1.180610\n",
      "iter 92500, loss: 1.101531\n",
      "iter 92600, loss: 1.030029\n",
      "iter 92700, loss: 0.963404\n",
      "iter 92800, loss: 0.903404\n",
      "iter 92900, loss: 0.847973\n",
      "----\n",
      " xpresses a single differesues a  otherene roduct and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score urdo a dy furms a dot product and optio \n",
      "----\n",
      "iter 93000, loss: 0.797185\n",
      "iter 93100, loss: 0.751124\n",
      "iter 93200, loss: 0.708223\n",
      "iter 93300, loss: 0.669740\n",
      "iter 93400, loss: 0.633777\n",
      "iter 93500, loss: 0.601637\n",
      "iter 93600, loss: 0.571376\n",
      "iter 93700, loss: 0.544291\n",
      "iter 93800, loss: 0.519771\n",
      "iter 93900, loss: 0.497143\n",
      "----\n",
      " : they are made up of neurons that have learnable weights abiofrnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The w \n",
      "----\n",
      "iter 94000, loss: 0.477128\n",
      "iter 94100, loss: 0.458051\n",
      "iter 94200, loss: 0.441219\n",
      "iter 94300, loss: 0.425392\n",
      "iter 94400, loss: 0.411390\n",
      "iter 94500, loss: 0.397949\n",
      "iter 94600, loss: 0.386271\n",
      "iter 94700, loss: 0.375391\n",
      "iter 94800, loss: 0.365317\n",
      "iter 94900, loss: 0.356525\n",
      "----\n",
      " rom the raw image pixels on one end to class scores fresses a sorms a l wious chapter: they are made up of neurons that al Neural Networks are very similar to ordinary Neural Networks from the previou \n",
      "----\n",
      "iter 95000, loss: 0.347868\n",
      "iter 95100, loss: 0.340430\n",
      "iter 95200, loss: 0.333155\n",
      "iter 95300, loss: 0.327075\n",
      "iter 95400, loss: 0.320835\n",
      "iter 95500, loss: 0.315677\n",
      "iter 95600, loss: 0.310800\n",
      "iter 95700, loss: 0.306366\n",
      "iter 95800, loss: 0.303141\n",
      "iter 95900, loss: 0.299515\n",
      "----\n",
      " weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearitw oneuravescon nave learnable weights and biases. Each neuron recearnable weig \n",
      "----\n",
      "iter 96000, loss: 0.297043\n",
      "iter 96100, loss: 0.294157\n",
      "iter 96200, loss: 0.292276\n",
      "iter 96300, loss: 0.289900\n",
      "iter 96400, loss: 0.288485\n",
      "iter 96500, loss: 0.286974\n",
      "iter 96600, loss: 0.285624\n",
      "iter 96700, loss: 0.284966\n",
      "iter 96800, loss: 0.283541\n",
      "iter 96900, loss: 0.283126\n",
      "----\n",
      " s at the other. And they still have a loss function: fune a loss function: from the raw image pixels on one exdiaulutiona s ond to chapter: they are made mathare up one enm to mitit stinas other. And  \n",
      "----\n",
      "iter 97000, loss: 0.282120\n",
      "iter 97100, loss: 0.282135\n",
      "iter 97200, loss: 0.281495\n",
      "iter 97300, loss: 0.281692\n",
      "iter 97400, loss: 0.281587\n",
      "iter 97500, loss: 0.281443\n",
      "iter 97600, loss: 0.281858\n",
      "iter 97700, loss: 0.281273\n",
      "iter 97800, loss: 0.281686\n",
      "iter 97900, loss: 0.281181\n",
      "----\n",
      " ts, performs a dot product and optionally follows it with a non-linearity. The whole networks a  neural Networks are very similar torm theverma to s apreurom ti harity function: from the raw image pix \n",
      "----\n",
      "iter 98000, loss: 0.281552\n",
      "iter 98100, loss: 0.280985\n",
      "iter 98200, loss: 0.281239\n",
      "iter 98300, loss: 0.281129\n",
      "iter 98400, loss: 0.280931\n",
      "iter 98500, loss: 0.281235\n",
      "iter 98600, loss: 0.280437\n",
      "iter 98700, loss: 0.280576\n",
      "iter 98800, loss: 0.279767\n",
      "iter 98900, loss: 0.279754\n",
      "----\n",
      " Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some imar \n",
      "----\n",
      "iter 99000, loss: 0.278757\n",
      "iter 99100, loss: 0.278559\n",
      "iter 99200, loss: 0.277981\n",
      "iter 99300, loss: 0.277240\n",
      "iter 99400, loss: 0.277009\n",
      "iter 99500, loss: 0.275762\n",
      "iter 99600, loss: 0.275344\n",
      "iter 99700, loss: 0.274145\n",
      "iter 99800, loss: 0.273716\n",
      "iter 99900, loss: 0.272419\n",
      "----\n",
      " it with a non-linearity. The whole network still er. Eagh neur. An have le ver: from the rons that have learnable weights and slverfut on-lineiraw eo have a loss functioliole upioa putm de metional Ne \n",
      "----\n",
      "iter 100000, loss: 0.271940\n",
      "iter 100100, loss: 0.271095\n",
      "iter 100200, loss: 0.270181\n",
      "iter 100300, loss: 0.269735\n",
      "iter 100400, loss: 0.268490\n",
      "iter 100500, loss: 0.268028\n",
      "iter 100600, loss: 0.266834\n",
      "iter 100700, loss: 0.266404\n",
      "iter 100800, loss: 0.265213\n",
      "iter 100900, loss: 0.264822\n",
      "----\n",
      " ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows  \n",
      "----\n",
      "iter 101000, loss: 0.264089\n",
      "iter 101100, loss: 0.263372\n",
      "iter 101200, loss: 0.263152\n",
      "iter 101300, loss: 0.262107\n",
      "iter 101400, loss: 0.261870\n",
      "iter 101500, loss: 0.260880\n",
      "iter 101600, loss: 0.260671\n",
      "iter 101700, loss: 0.259642\n",
      "iter 101800, loss: 0.259440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 101900, loss: 0.258875\n",
      "----\n",
      " xpresses a single differentiable score function: from the raw ils frnm thes on one end to class scores at the other. And they still have a loss function: fry sime hevlear itiagious chapter: they are f \n",
      "----\n",
      "iter 102000, loss: 0.258313\n",
      "iter 102100, loss: 0.258227\n",
      "iter 102200, loss: 0.257292\n",
      "iter 102300, loss: 0.257169\n",
      "iter 102400, loss: 0.256292\n",
      "iter 102500, loss: 0.256174\n",
      "iter 102600, loss: 0.255203\n",
      "iter 102700, loss: 0.255044\n",
      "iter 102800, loss: 0.254492\n",
      "iter 102900, loss: 0.253896\n",
      "----\n",
      " : they are made up of neurons that have learnable weights and biases. Each neuron receives soutias on one end to class scores at the other. And they still have learom the previous chapter: they are ma \n",
      "----\n",
      "iter 103000, loss: 0.254141\n",
      "iter 103100, loss: 0.839529\n",
      "iter 103200, loss: 1.343901\n",
      "iter 103300, loss: 1.704827\n",
      "iter 103400, loss: 1.738844\n",
      "iter 103500, loss: 1.619490\n",
      "iter 103600, loss: 1.499488\n",
      "iter 103700, loss: 1.387444\n",
      "iter 103800, loss: 1.284743\n",
      "iter 103900, loss: 1.191267\n",
      "----\n",
      " rom the raw il  ot  nf ron receives some inputs, performs a dot product and optionally futherentiable score th neuron uts, performs a dot product and optionally follows it with a non-linearity. The wh \n",
      "----\n",
      "iter 104000, loss: 1.105225\n",
      "iter 104100, loss: 1.027364\n",
      "iter 104200, loss: 0.955690\n",
      "iter 104300, loss: 0.891033\n",
      "iter 104400, loss: 0.831304\n",
      "iter 104500, loss: 0.777614\n",
      "iter 104600, loss: 0.728307\n",
      "iter 104700, loss: 0.683401\n",
      "iter 104800, loss: 0.643161\n",
      "iter 104900, loss: 0.605701\n",
      "----\n",
      " weights and biases. Each neuron receiver sof ner. And thes. Eachavery from the previous chapter: they are made up of neurons that have leives fanetwork still expresses a single differentiable score fu \n",
      "----\n",
      "iter 105000, loss: 0.572137\n",
      "iter 105100, loss: 0.541131\n",
      "iter 105200, loss: 0.513171\n",
      "iter 105300, loss: 0.487259\n",
      "iter 105400, loss: 0.464182\n",
      "iter 105500, loss: 0.442636\n",
      "iter 105600, loss: 0.423265\n",
      "iter 105700, loss: 0.405730\n",
      "iter 105800, loss: 0.389226\n",
      "iter 105900, loss: 0.374718\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
